\documentclass[11pt,a4paper]{article}

% ========================================
% PACKAGES
% ========================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}  % Change to your language
\usepackage[margin=2.5cm]{geometry}

% Mathematics
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}

% Graphics and colors
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}

% Lists and formatting
\usepackage{enumitem}
\usepackage{parskip}
\usepackage{fancyhdr}

% Code listings (if needed)
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% ========================================
% THEOREM ENVIRONMENTS
% ========================================
\theoremstyle{definition}
\newtheorem{definition}{Definizione}[section]
\newtheorem{example}{Esempio}[section]
\newtheorem{exercise}{Esercizio}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Teorema}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposizione}
\newtheorem{corollary}[theorem]{Corollario}

\theoremstyle{remark}
\newtheorem*{remark}{Nota}
\newtheorem*{observation}{Osservazione}

% ========================================
% CUSTOM COMMANDS
% ========================================
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

% ========================================
% HEADER AND FOOTER
% ========================================
\pagestyle{fancy}
\setlength{\headheight}{14pt}
\fancyhf{}
\lhead{AR}
\rhead{AR}
\cfoot{\thepage}

% ========================================
% DOCUMENT INFORMATION
% ========================================
\title{\textbf{Automated Reasoning}\\
\large Artificial Intelligence}
\author{Jacopo Parretti}
\date{I Semester 2025-2026}

% ========================================
% DOCUMENT
% ========================================
\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\part{}


\section{Introduction}

\subsection{Why Study Automated Reasoning?}

Automated Reasoning is a fundamental field of artificial intelligence concerned with the design and implementation of computational procedures that enable machines to solve problems formulated in logical languages. These procedures involve \textbf{logical inference and search}, allowing systems to reason in a deductive manner through mechanical processes.

Since the inception of artificial intelligence at the Dartmouth Conference in 1956, one of the core challenges has been to demonstrate machine intelligence through the automated proving of mathematical and logical theorems. This capability represents a cornerstone of symbolic AI, where knowledge is represented symbolically and stored in computer memory, and systematic procedures are applied to solve complex problems.

\textbf{Theorem proving} traditionally requires human-level mathematical reasoning and insight. The goal of automated reasoning is to mechanize this process, creating algorithms and systems capable of performing logical deduction without human intervention.

A logical language consists of formal symbols and syntactic rules. Within the symbolic approach to artificial intelligence, knowledge is encoded using these symbols, which are stored in computer memory. The system then applies a sequence of mechanical steps to manipulate these symbols and arrive at solutions to the given problems.



\newpage

\section{Historical Foundations and Theoretical Limits}

\subsection{The Hilbert-Turing Perspective}

The foundations of automated reasoning can be traced to fundamental questions in mathematical logic and computability. Since Alan Turing's seminal work on \textit{Turing machines} in 1936, researchers have pursued David Hilbert's 1900 challenge: is it possible to develop a mechanical procedure---executed deterministically by a human computor---capable of solving any mathematical problem?

\textbf{The Entscheidungsproblem}: A central question that emerged from this pursuit asks whether it is possible to define a mechanical method to determine the validity of formulas in first-order logic.

Turing demonstrated that the halting problem is undecidable using Turing machines. This fundamental result can be reduced to the validity problem in first-order logic, providing a negative answer to Hilbert's challenge. This undecidability result forms one of the core theoretical foundations of automated reasoning, establishing fundamental limits on what can be mechanically computed.

\subsection{Practical Applications of Automated Reasoning}

Automated reasoning systems find application across multiple domains requiring formal verification and systematic problem-solving:

\begin{itemize}
    \item \textbf{Planning}: Given an initial state, goal state, and formal model of agent actions, determine a sequence of moves that transforms the initial state into the goal state (utilizing SAT solvers)
    \item \textbf{Scheduling}: Generate temporal arrangements of tasks that satisfy complex temporal and resource constraints
    \item \textbf{Software Analysis and Verification}: Formal methods for ensuring program correctness, including:
        \begin{itemize}
            \item Provable privacy guarantees in security protocols
            \item Verification of distributed algorithms
            \item Analysis of randomized algorithms and their probabilistic properties
        \end{itemize}
    \item \textbf{Hybrid Approaches}: Integration of automated reasoning with machine learning techniques for enhanced problem-solving capabilities
\end{itemize}

These applications demonstrate the practical utility of automated reasoning in solving complex real-world problems that require rigorous logical analysis and systematic exploration of solution spaces.


\section{Core Problems in Automated Reasoning}

Automated reasoning fundamentally addresses three interconnected classes of decision problems in mathematical logic, each representing a distinct challenge in determining the truth properties of logical formulas.

\subsection{Satisfiability Problems}

A satisfiability problem (SAT) concerns determining whether a given logical formula has a truth assignment that makes it true. Formally, for a formula $\phi$ with free variables, we seek to determine if there exists an interpretation $\mathcal{I}$ such that $\mathcal{I} \models \phi$. This represents the existence problem---does there exist a model making the formula true?

\subsection{Validity Problems}

In contrast to satisfiability, validity problems ask whether a formula holds true under all possible interpretations. A formula $\phi$ is valid (denoted $\models \phi$) if every possible truth assignment to its variables results in $\phi$ evaluating to true. This corresponds to the universal quantification over all possible models---the formula must hold in every conceivable scenario.

\subsection{Logical Consequence Problems}

The logical consequence problem represents a generalization of validity, asking whether a conclusion necessarily follows from given premises. Given a set of assumptions $\Gamma = \{\phi_1, \phi_2, \dots, \phi_n\}$ and a conjecture $\psi$, we determine if $\Gamma \models \psi$ (i.e., if $\psi$ is true in every model where all formulas in $\Gamma$ are true). This captures the essence of logical deduction---what conclusions are forced by the given information.

These three problems form the foundation of automated reasoning systems, with satisfiability serving as the most tractable and finding widespread application in practical problem-solving contexts. 


\section{Formal Language and Semantics}

\subsection{Logical Language: First-Order Logic}

The formal language employed in automated reasoning is first-order logic (FOL), a substantial extension of propositional logic that incorporates quantification over individual elements. First-order logic achieves greater expressive power by introducing:

\begin{itemize}
    \item \textbf{Individual variables} ranging over domain elements
    \item \textbf{Function symbols} for constructing complex terms
    \item \textbf{Predicate symbols} for expressing relations between elements
    \item \textbf{Quantifiers} ($\forall$, $\exists$) enabling statements about all or some elements
\end{itemize}

This enhanced expressiveness allows first-order logic to formalize mathematical structures, relationships, and properties in a manner impossible in propositional logic, which can only express truth-functional combinations of atomic propositions.

\subsection{Signature and Universe of Discourse}

A signature $\Sigma$ provides the vocabulary for interpreting formulas within a specific domain. Formally, a signature is a triple $\Sigma = \langle S, \mathcal{F}, \mathcal{P} \rangle$ where:

\begin{itemize}
    \item $S$ is a set of sorts (disjoint domains of interpretation)
    \item $\mathcal{F}$ is a set of function symbols, each with an arity specifying the number of arguments
    \item $\mathcal{P}$ is a set of predicate symbols, which we use to write formulas, each with an associated arity
\end{itemize}

The signature defines the syntactic elements available for constructing formulas, while the universe of discourse (or domain) provides the semantic interpretation. Sorts correspond to types in programming languages, enabling the formal specification of heterogeneous domains with different categories of objects.

The signature $\Sigma = \langle S, \mathcal{F}, \mathcal{P} \rangle$ can be further decomposed to provide a complete syntactic framework. The set of function symbols $\mathcal{F}$ is typically partitioned into two distinct subsets:

\begin{itemize}
    \item \textbf{Constant symbols} $C \subseteq \mathcal{F}$: These are function symbols of arity 0, representing named individuals in the domain. Constants have fixed interpretations and serve as proper names for specific domain elements.

    \item \textbf{Proper function symbols} $\mathcal{F} \setminus C$: These represent actual functions mapping tuples of domain elements to domain elements, each with arity $n \geq 1$.
\end{itemize}

Formally, a function symbol $f \in \mathcal{F}$ has type $s_1 \times s_2 \times \cdots \times s_n \rightarrow s$, where $s_1, \dots, s_n, s \in S$ are sorts indicating the domain and codomain of the function. For example:
\begin{itemize}
    \item $c : \rightarrow \texttt{int}$ (a constant symbol denoting a specific integer)
    \item $p : \rightarrow \texttt{color}$ (a constant symbol denoting a specific color)
    \item $f : \texttt{int} \times \texttt{int} \rightarrow \texttt{int}$ (a binary function like addition)
\end{itemize}

This decomposition enables precise specification of both individual elements (via constants) and functional relationships (via proper function symbols) within the formal language.

The complete signature structure $\langle S, C \cup (\mathcal{F} \setminus C) \cup \mathcal{P} \rangle$ provides the full syntactic foundation for expressing complex logical statements about structured domains.

Constants can be viewed as function symbols of arity 0---functions that require no arguments and directly denote specific domain elements. Additionally, to construct quantified formulas and express generality, we require a countably infinite set of variables $X = \{x, y, z, \dots\}$ disjoint from the signature symbols. 


Thus, the function symbols may be comprehensively categorized as $\mathcal{F} = C \cup (\mathcal{F} \setminus C) \cup \mathcal{P}$, where $\mathcal{P}$ represents the set of predicate symbols. Each predicate symbol $p \in \mathcal{P}$ has arity $n \geq 0$ and type $s_1 \times s_2 \times \cdots \times s_n$, where the final sort represents the truth value (typically a boolean sort).

Predicate symbols differ fundamentally from function symbols in that they represent relations rather than functions---they evaluate to truth values rather than domain elements. For example:
\begin{itemize}
    \item $even : \texttt{int} \rightarrow \texttt{bool}$ (unary predicate testing parity)
    \item $less : \texttt{int} \times \texttt{int} \rightarrow \texttt{bool}$ (binary relation for ordering)
\end{itemize}

The complete signature structure $\langle S, C \cup (\mathcal{F} \setminus C) \cup \mathcal{P} \rangle$ provides the full syntactic foundation for expressing complex logical statements about structured domains.

Constants can be viewed as function symbols of arity 0---functions that require no arguments and directly denote specific domain elements. Additionally, to construct quantified formulas and express generality, we require a countably infinite set of variables $X = \{x, y, z, \dots\}$ disjoint from the signature symbols.

\section{Syntactic Foundations: Terms and Formulas}

\subsection{Terms: The Building Blocks}

The most fundamental syntactic objects in first-order logic are \textbf{terms}, which represent individuals in the domain of discourse. The set of terms $\mathcal{T}_\Sigma(X)$ over signature $\Sigma$ and variables $X$ is inductively defined as the smallest set satisfying:

\begin{itemize}
    \item \textbf{Constants}: If $c \in C$ is a constant symbol, then $c \in \mathcal{T}_\Sigma(X)$
    \item \textbf{Variables}: If $x \in X$ is a variable, then $x \in \mathcal{T}_\Sigma(X)$
    \item \textbf{Function application}: If $f \in \mathcal{F}$ is a function symbol of arity $n \geq 1$ and $t_1, \dots, t_n \in \mathcal{T}_\Sigma(X)$ are terms, then $f(t_1, \dots, t_n) \in \mathcal{T}_\Sigma(X)$
\end{itemize}

Terms can be conceptualized as finite trees where:
\begin{itemize}
    \item Leaf nodes are constants or variables
    \item Internal nodes are function symbols applied to subterms
\end{itemize}

For example, the term $f(x, g(a), z)$ has the tree representation:
\[
\begin{array}{c}
  f \\
  | \\
  \begin{array}{ccc}
    x & g & z \\
       & | & \\
       & a &
  \end{array}
\end{array}
\]

Terms may contain subterms, which correspond to subtrees in this representation. Complex terms are constructed recursively from simpler subterms, reflecting the hierarchical nature of the syntactic structure.

Terms may contain subterms, which correspond to subtrees in this representation. Complex terms are constructed recursively from simpler subterms, reflecting the hierarchical nature of the syntactic structure.

\subsection{From Terms to Atomic Formulas}

Having established the notion of terms as representations of individuals, we now introduce atomic formulas to express relationships between these individuals. An \textbf{atomic formula} (or \textbf{atom}) is constructed by applying a predicate symbol to an appropriate number of terms.

Formally, if $P \in \mathcal{P}$ is a predicate symbol of arity $n \geq 0$ and $t_1, \dots, t_n \in \mathcal{T}_\Sigma(X)$ are terms, then $P(t_1, \dots, t_n)$ is an atomic formula. When $n = 0$, the atomic formula reduces to the predicate symbol itself $P$, which can be interpreted as a propositional constant.

The truth value of an atomic formula $P(t_1, \dots, t_n)$ depends on the interpretation of the predicate $P$ and the denotations of the terms $t_1, \dots, t_n$ in a given model.

\subsection{Literals: Positive and Negative Atomic Formulas}

A \textbf{literal} is either an atomic formula (positive literal) or its negation (negative literal). Formally, if $\phi$ is an atomic formula, then:
\begin{itemize}
    \item $\phi$ is a positive literal
    \item $\neg \phi$ is a negative literal
\end{itemize}

Literals represent the basic building blocks for constructing more complex logical formulas and play a crucial role in automated reasoning systems, particularly in resolution-based theorem proving.

\subsection{Logical Connectives}

Logical connectives provide the means to construct complex formulas from simpler ones. These operators enable the expression of compound logical statements through systematic combination of atomic formulas and literals.

The primary logical connectives include:
\begin{itemize}
    \item \textbf{Negation} ($\neg$): Expresses logical denial or contradiction
    \item \textbf{Conjunction} ($\wedge$): Represents logical conjunction (AND)
    \item \textbf{Disjunction} ($\vee$): Represents logical disjunction (OR)
    \item \textbf{Implication} ($\rightarrow$): Expresses logical implication (IF...THEN)
    \item \textbf{Biconditional} ($\leftrightarrow$): Represents logical equivalence (IF AND ONLY IF)
\end{itemize}

\subsection{Formulas: The Complete Syntax}

The set of \textbf{formulas} (or \textbf{well-formed formulas}) $\mathcal{F}_\Sigma(X)$ over signature $\Sigma$ and variables $X$ is inductively defined as the smallest set satisfying:

\begin{itemize}
    \item \textbf{Atomic formulas}: If $P \in \mathcal{P}$ is a predicate symbol of arity $n \geq 0$ and $t_1, \dots, t_n \in \mathcal{T}_\Sigma(X)$, then $P(t_1, \dots, t_n) \in \mathcal{F}_\Sigma(X)$

    \item \textbf{Negation}: If $\phi \in \mathcal{F}_\Sigma(X)$, then $(\neg \phi) \in \mathcal{F}_\Sigma(X)$

    \item \textbf{Binary connectives}: If $\phi, \psi \in \mathcal{F}_\Sigma(X)$, then:
        \begin{itemize}
            \item $(\phi \wedge \psi) \in \mathcal{F}_\Sigma(X)$ (conjunction)
            \item $(\phi \vee \psi) \in \mathcal{F}_\Sigma(X)$ (disjunction)
            \item $(\phi \rightarrow \psi) \in \mathcal{F}_\Sigma(X)$ (implication)
            \item $(\phi \leftrightarrow \psi) \in \mathcal{F}_\Sigma(X)$ (biconditional)
        \end{itemize}
\end{itemize}

This inductive definition ensures that all syntactically valid logical expressions can be systematically constructed from the basic building blocks of atomic formulas through the application of logical connectives. 

This inductive definition ensures that all syntactically valid logical expressions can be systematically constructed from the basic building blocks of atomic formulas through the application of logical connectives.

\subsection{Quantifiers: Universality and Existence}

To complete the syntax of first-order logic, we introduce quantifiers that express generality and existence over the domain of discourse:

\begin{itemize}
    \item \textbf{Universal quantifier} ($\forall$): If $\phi \in \mathcal{F}_\Sigma(X)$ is a formula and $x \in X$ is a variable, then $(\forall x \, \phi) \in \mathcal{F}_\Sigma(X)$
    \item \textbf{Existential quantifier} ($\exists$): If $\phi \in \mathcal{F}_\Sigma(X)$ is a formula and $x \in X$ is a variable, then $(\exists x \, \phi) \in \mathcal{F}_\Sigma(X)$
\end{itemize}

Quantifiers bind variables within their scope, enabling the expression of properties that hold for all elements of the domain (universal quantification) or for at least one element (existential quantification).

\subsection{Illustrative Example: Geometric Axioms}

Consider the following first-order formula expressing that any two distinct points determine a unique line:

\[
\forall x \, \forall y \, \left( (P(x) \wedge P(y) \wedge x \neq y) \rightarrow \exists z \, (L(z) \wedge Q(z, x, y) \wedge \forall w \, (L(w) \wedge Q(w, x, y) \rightarrow w = z)) \right)
\]

Where:
\begin{itemize}
    \item $P(x)$ denotes that $x$ is a point
    \item $L(z)$ denotes that $z$ is a line
    \item $Q(z, x, y)$ denotes that line $z$ passes through points $x$ and $y$
    \item $x \neq y$ expresses that points $x$ and $y$ are distinct
\end{itemize}

This axiom formalizes the geometric principle that given any two distinct points in space, there exists exactly one line passing through both points, capturing a fundamental property of Euclidean geometry within the framework of first-order logic. 

This axiom formalizes the geometric principle that given any two distinct points in space, there exists exactly one line passing through both points, capturing a fundamental property of Euclidean geometry within the framework of first-order logic.

\subsection{Closed Formulas and Variable Scope}

A \textbf{closed formula} (or \textbf{sentence}) is a formula containing no free variables---all variables are bound by quantifiers. A variable is \textbf{free} in a formula if it appears outside the scope of any quantifier that binds it; conversely, a variable is \textbf{bound} if it appears within the scope of a quantifier.

Formulas may contain both free and bound occurrences of variables. For proper semantic interpretation, variables must be \textbf{standardized apart}, meaning that no variable appears both free and bound in the same formula, and distinct quantifiers use distinct bound variables.



\newpage

\part{Semantic Foundations}

\section{Interpretations and Models}

\subsection{The Concept of Interpretation}

To assign meaning to syntactic formulas in first-order logic, we require a formal notion of \textbf{interpretation}. An interpretation $\mathcal{I}$ provides a semantic mapping from the syntactic elements of a signature to concrete mathematical objects.

In the unsorted case, an interpretation consists of:
\begin{itemize}
    \item A non-empty set $D$ called the \textbf{domain} (or universe of discourse)
    \item An interpretation function $\Phi$ that maps:
    \begin{itemize}
        \item Each predicate symbol $P \in \mathcal{P}$ of arity $n$ to a subset $\Phi(P) \subseteq D^n$ (representing an $n$-ary relation over the domain)
        \item Each function symbol $f \in \mathcal{F}$ of arity $n$ to a function $\Phi(f) : D^n \rightarrow D$
        \item Each constant symbol $c \in C$ to a specific element $\Phi(c) \in D$
    \end{itemize}
\end{itemize}

The interpretation function $\Phi$ bridges the gap between syntax and semantics, transforming abstract symbols into concrete mathematical entities.

\subsection{Example: Interpreting a Formula over Natural Numbers}

Consider the first-order formula:
\[
\forall x \, (\neg P(x) \rightarrow P(f(x)))
\]

This formula contains:
\begin{itemize}
    \item A unary predicate symbol $P$
    \item A unary function symbol $f$
    \item A universally quantified variable $x$
\end{itemize}

Since the formula is closed (contains no free variables), its truth value depends entirely on the chosen interpretation.

\textbf{Constructing an interpretation}: Let us define an interpretation $\mathcal{I} = \langle D, \Phi \rangle$ where:
\begin{itemize}
    \item \textbf{Domain}: $D = \N$ (the set of natural numbers)
    \item \textbf{Function interpretation}: $\Phi(f) : \N \rightarrow \N$ is the successor function, defined by $\Phi(f)(n) = n + 1$ for all $n \in \N$
    \item \textbf{Predicate interpretation}: $\Phi(P) = \{0, 2, 4, 6, \dots\} = \{2k \mid k \in \N\}$ (the set of even natural numbers)
\end{itemize}

\subsection{Semantic Evaluation}

Under this interpretation $\mathcal{I}$, the formula $\forall x \, (\neg P(x) \rightarrow P(f(x)))$ receives the following semantic reading:

\textit{For all natural numbers $x$, if $x$ is not even, then the successor of $x$ is even.}

To determine whether this formula is true in interpretation $\mathcal{I}$, we must verify whether the implication holds for every element of the domain:
\begin{itemize}
    \item If $x$ is odd (i.e., $x \notin \Phi(P)$), then $x + 1$ must be even (i.e., $x + 1 \in \Phi(P)$)
\end{itemize}

This statement is indeed true in the natural numbers: the successor of any odd number is even. Therefore, the formula is \textbf{satisfied} by interpretation $\mathcal{I}$, and we write $\mathcal{I} \models \forall x \, (\neg P(x) \rightarrow P(f(x)))$.

\subsection{Formal Verification of Satisfaction}

To formally verify that $\mathcal{I} \models \forall x \, (\neg P(x) \rightarrow P(f(x)))$, we must check that for all $n \in \N$, the interpretation satisfies the implication with the substitution $\{x \leftarrow n\}$.

By the semantics of implication, $\mathcal{I}$ satisfies $(\neg P(x) \rightarrow P(f(x)))$ with assignment $\{x \leftarrow n\}$ if and only if:
\begin{itemize}
    \item Either $\mathcal{I}$ does not satisfy $\neg P(x)$ with $\{x \leftarrow n\}$ (i.e., $\mathcal{I}$ satisfies $P(x)$ with $\{x \leftarrow n\}$), or
    \item $\mathcal{I}$ satisfies $P(f(x))$ with $\{x \leftarrow n\}$
\end{itemize}

Equivalently, for all $n \in \N$, either $n \in \Phi(P)$ or $\Phi(f)(n) \in \Phi(P)$.

\textbf{Truth condition for atomic formulas}: An atomic formula $P(t)$ is true under interpretation $\mathcal{I}$ if and only if the interpretation of the term $t$ belongs to the interpretation of the predicate $\Phi(P)$.

In our example, for all $n \in \N$:
\begin{itemize}
    \item Either $n \in \Phi(P)$ (meaning $n$ is even), or
    \item $\Phi(f)(n) = n + 1 \in \Phi(P)$ (meaning the successor of $n$ is even)
\end{itemize}

This condition holds for all natural numbers, confirming that the formula is satisfied by the interpretation.

\subsection{Formal Definition of Interpretation}

Having illustrated the concept through examples, we now provide the complete formal definition of an interpretation in first-order logic.

An \textbf{interpretation} $\mathcal{I}$ consists of a triple $\mathcal{I} = \langle D, \Phi, \beta \rangle$ where:

\begin{itemize}
    \item \textbf{Domain}: $D$ is a non-empty set representing the universe of discourse
    
    \item \textbf{Interpretation function} $\Phi$ maps signature symbols to semantic objects:
    \begin{itemize}
        \item For each constant symbol $c \in C$: $\Phi(c) = d \in D$
        \item For each function symbol $f \in \mathcal{F}$ of arity $n \geq 1$: $\Phi(f) : D^n \rightarrow D$, where $D^n$ denotes the $n$-fold Cartesian product $D \times D \times \cdots \times D$
        \item For each predicate symbol $P \in \mathcal{P}$ of arity $n \geq 0$: $\Phi(P) \subseteq D^n$ (an $n$-ary relation over $D$)
    \end{itemize}
    
    \item \textbf{Variable assignment} $\beta$ maps each variable to a domain element:
    \begin{itemize}
        \item $\beta : X \rightarrow D$ such that $\beta(x) = d \in D$ for all variables $x \in X$
    \end{itemize}
\end{itemize}

\textbf{Assignment update notation}: Given an assignment $\beta$ and a substitution $x \leftarrow d$, we define the updated assignment $\beta[x \leftarrow d]$ as:
\[
\beta[x \leftarrow d](y) = 
\begin{cases}
d & \text{if } y = x \\
\beta(y) & \text{otherwise}
\end{cases}
\]

This notation allows us to formally express the evaluation of formulas with different variable bindings, which is essential for defining the semantics of quantified formulas.






\section{Satisfaction Relation}

\subsection{The Satisfaction Relation}

Having defined interpretations and variable assignments, we now formalize the central semantic concept: when does an interpretation \textbf{satisfy} a formula? The satisfaction relation, denoted $\mathcal{I} \models_\beta \phi$, specifies the conditions under which a formula $\phi$ is true in interpretation $\mathcal{I}$ with variable assignment $\beta$.

Let $F$ and $G$ denote arbitrary formulas. The satisfaction relation is defined inductively on the structure of formulas.

\subsection{Interpretation of Terms}

Before defining satisfaction for formulas, we must specify how terms are interpreted. Given an interpretation $\mathcal{I} = \langle D, \Phi, \beta \rangle$, the interpretation of a term $t$, denoted $\Phi_\beta(t)$, is defined recursively:

\begin{itemize}
    \item \textbf{Constants}: $\Phi_\beta(c) = \Phi(c) \in D$
    \item \textbf{Variables}: $\Phi_\beta(x) = \beta(x) \in D$
    \item \textbf{Function application}: $\Phi_\beta(f(t_1, \dots, t_n)) = \Phi(f)(\Phi_\beta(t_1), \dots, \Phi_\beta(t_n)) \in D$
\end{itemize}

Each term evaluates to a specific element of the domain under a given interpretation.

\subsection{Satisfaction for Atomic Formulas}

\textbf{Base case} (atomic formulas): An interpretation $\mathcal{I}$ satisfies an atomic formula $P(t_1, \dots, t_n)$ under assignment $\beta$ if and only if the tuple of interpreted terms belongs to the interpretation of the predicate:

\[
\mathcal{I} \models_\beta P(t_1, \dots, t_n) \quad \text{iff} \quad (\Phi_\beta(t_1), \dots, \Phi_\beta(t_n)) \in \Phi(P)
\]

\subsection{Satisfaction for Logical Connectives}

The satisfaction relation extends to compound formulas through the following inductive clauses. Note that all satisfaction relations are parameterized by the variable assignment $\beta$, written as $\models_\beta$.

\begin{itemize}
    \item \textbf{Negation}: 
    \[
    \mathcal{I} \models_\beta \neg F \quad \text{iff} \quad \mathcal{I} \not\models_\beta F
    \]
    
    \item \textbf{Conjunction}: 
    \[
    \mathcal{I} \models_\beta (F \wedge G) \quad \text{iff} \quad \mathcal{I} \models_\beta F \text{ and } \mathcal{I} \models_\beta G
    \]
    
    \item \textbf{Disjunction}: 
    \[
    \mathcal{I} \models_\beta (F \vee G) \quad \text{iff} \quad \mathcal{I} \models_\beta F \text{ or } \mathcal{I} \models_\beta G
    \]
    
    \item \textbf{Implication}: 
    \[
    \mathcal{I} \models_\beta (F \rightarrow G) \quad \text{iff} \quad \mathcal{I} \not\models_\beta F \text{ or } \mathcal{I} \models_\beta G
    \]
    Equivalently: if $\mathcal{I} \models_\beta F$ then $\mathcal{I} \models_\beta G$
    
    \item \textbf{Biconditional}: 
    \[
    \mathcal{I} \models_\beta (F \leftrightarrow G) \quad \text{iff} \quad \mathcal{I} \models_\beta F \text{ if and only if } \mathcal{I} \models_\beta G
    \]
\end{itemize}

\subsection{Satisfaction for Quantified Formulas}

Quantifiers introduce variable bindings that range over the entire domain:

\begin{itemize}
    \item \textbf{Universal quantification}: 
    \[
    \mathcal{I} \models_\beta \forall x \, G \quad \text{iff} \quad \text{for all } d \in D, \, \mathcal{I} \models_{\beta[x \leftarrow d]} G
    \]
    The formula $\forall x \, G$ is satisfied if $G$ is satisfied under every possible assignment to $x$.
    
    \item \textbf{Existential quantification}: 
    \[
    \mathcal{I} \models_\beta \exists x \, G \quad \text{iff} \quad \text{there exists } d \in D \text{ such that } \mathcal{I} \models_{\beta[x \leftarrow d]} G
    \]
    The formula $\exists x \, G$ is satisfied if $G$ is satisfied under at least one assignment to $x$.
\end{itemize}

This completes the inductive definition of the satisfaction relation, providing a complete semantics for first-order logic.

\subsection{Example: Quantifier Evaluation}

Consider the formula $\forall x \, \exists y \, R(x, y)$, which can be read as "for all $x$, there exists $y$ such that $x < y$".

Let $\mathcal{I} = \langle \N, \Phi \rangle$ where $\Phi(R) = \{(n, m) \in \N \times \N \mid n < m\}$ (the standard ordering on natural numbers).

To verify $\mathcal{I} \models \forall x \, \exists y \, R(x, y)$, we apply the semantic definitions:
\begin{align*}
\mathcal{I} \models \forall x \, \exists y \, R(x, y) 
&\quad \text{iff} \quad \text{for all } n \in \N, \, \mathcal{I} \models_{\beta[x \leftarrow n]} \exists y \, R(x, y) \\
&\quad \text{iff} \quad \text{for all } n \in \N, \text{ there exists } m \in \N \text{ such that } \mathcal{I} \models_{\beta[x \leftarrow n, y \leftarrow m]} R(x, y) \\
&\quad \text{iff} \quad \text{for all } n \in \N, \text{ there exists } m \in \N \text{ such that } (n, m) \in \Phi(R) \\
&\quad \text{iff} \quad \text{for all } n \in \N, \text{ there exists } m \in \N \text{ such that } n < m
\end{align*}

This statement is \textbf{false} in $\N$ because there is no natural number greater than all natural numbers. However, it would be true in $\mathbb{Z}$ or $\mathbb{Q}$, demonstrating how truth depends on the chosen interpretation.

\section{Semantic Properties of Formulas}

Having established the satisfaction relation, we now define fundamental semantic properties that classify formulas according to their truth behavior across different interpretations.

\subsection{Satisfiability}

A formula $F$ is \textbf{satisfiable} if there exists an interpretation $\mathcal{I}$ such that $\mathcal{I} \models F$. In this case, we say that $\mathcal{I}$ is a \textbf{model} of $F$.

\[
F \text{ is satisfiable} \quad \Leftrightarrow \quad \exists \mathcal{I} : \mathcal{I} \models F
\]

\subsection{Validity}

A formula $F$ is \textbf{valid} (or a \textbf{tautology}) if for all interpretations $\mathcal{I}$, we have $\mathcal{I} \models F$. Valid formulas are true in every possible interpretation.

\[
F \text{ is valid} \quad \Leftrightarrow \quad \forall \mathcal{I} : \mathcal{I} \models F
\]

We denote validity by $\models F$.

\subsection{Unsatisfiability}

A formula $F$ is \textbf{unsatisfiable} (or \textbf{contradictory}) if there exists no interpretation $\mathcal{I}$ such that $\mathcal{I} \models F$. Unsatisfiable formulas are false in every interpretation.

\[
F \text{ is unsatisfiable} \quad \Leftrightarrow \quad \nexists \mathcal{I} : \mathcal{I} \models F \quad \Leftrightarrow \quad \forall \mathcal{I} : \mathcal{I} \not\models F
\]

\subsection{Invalidity}

A formula $F$ is \textbf{invalid} (or \textbf{falsifiable}) if there exists an interpretation $\mathcal{I}$ such that $\mathcal{I} \not\models F$. In this case, we say that $\mathcal{I}$ is a \textbf{countermodel} (or \textbf{counterexample}) for $F$.

\[
F \text{ is invalid} \quad \Leftrightarrow \quad \exists \mathcal{I} : \mathcal{I} \not\models F
\]

\subsection{Relationships Between Semantic Properties}

These semantic properties are intimately related through negation:

\begin{theorem}[Validity and Unsatisfiability]
A formula $F$ is valid if and only if $\neg F$ is unsatisfiable.
\end{theorem}

\begin{proof}
Suppose $F$ is valid. Then for all interpretations $\mathcal{I}$, we have $\mathcal{I} \models F$, which implies $\mathcal{I} \not\models \neg F$. Therefore, $\neg F$ is unsatisfiable.

Conversely, if $\neg F$ is unsatisfiable, then for all interpretations $\mathcal{I}$, we have $\mathcal{I} \not\models \neg F$, which implies $\mathcal{I} \models F$. Therefore, $F$ is valid.
\end{proof}

\begin{theorem}[Satisfiability and Invalidity]
A formula $F$ is satisfiable if and only if $\neg F$ is invalid.
\end{theorem}

\begin{proof}
$F$ is satisfiable if and only if there exists an interpretation $\mathcal{I}$ such that $\mathcal{I} \models F$, which is equivalent to $\mathcal{I} \not\models \neg F$. This is precisely the condition for $\neg F$ to be invalid.
\end{proof}

These relationships demonstrate the duality between validity and unsatisfiability, and between satisfiability and invalidity, providing a complete characterization of the semantic landscape of first-order logic.

\section{Logical Consequence}

\subsection{The Three Fundamental Query Types}

Automated reasoning systems must be capable of resolving three fundamental types of semantic queries:
\begin{enumerate}
    \item \textbf{Satisfiability queries}: Is formula $F$ satisfiable?
    \item \textbf{Validity queries}: Is formula $F$ valid?
    \item \textbf{Logical consequence queries}: Does $F$ follow logically from a set of hypotheses $H$?
\end{enumerate}

\subsection{Definition of Logical Consequence}

Let $H = \{\phi_1, \phi_2, \dots, \phi_n\}$ be a set of formulas (the \textbf{hypotheses} or \textbf{premises}) and let $F$ be a formula (the \textbf{conclusion}). We say that $F$ is a \textbf{logical consequence} of $H$, written $H \models F$, and read as "$H$ entails $F$" or "$F$ follows logically from $H$", if:

\[
H \models F \quad \Leftrightarrow \quad \text{for all interpretations } \mathcal{I}, \text{ if } \mathcal{I} \models H \text{ then } \mathcal{I} \models F
\]

In other words, $F$ is a logical consequence of $H$ if every model of $H$ is also a model of $F$. Here, $\mathcal{I} \models H$ means that $\mathcal{I}$ satisfies all formulas in $H$.

\subsection{The Deduction Theorem}

The deduction theorem establishes a fundamental connection between logical consequence and validity, reducing the problem of checking entailment to the problem of checking validity.

\begin{theorem}[Deduction Theorem]
Let $H$ be a set of formulas and $F$ be a formula. Then:
\[
H \models F \quad \text{iff} \quad \models (H \rightarrow F)
\]
where $H \rightarrow F$ denotes the implication from the conjunction of all formulas in $H$ to $F$.
\end{theorem}

\begin{proof}
We prove both directions:

\textbf{($\Rightarrow$)} Assume $H \models F$. We must show that $H \rightarrow F$ is valid, i.e., for all interpretations $\mathcal{I}$, we have $\mathcal{I} \models H \rightarrow F$.

Let $\mathcal{I}$ be an arbitrary interpretation. By the semantics of implication, $\mathcal{I} \models H \rightarrow F$ holds if either:
\begin{itemize}
    \item $\mathcal{I} \not\models H$ (the antecedent is false), or
    \item $\mathcal{I} \models F$ (the consequent is true)
\end{itemize}

If $\mathcal{I} \not\models H$, then $\mathcal{I} \models H \rightarrow F$ trivially. If $\mathcal{I} \models H$, then by the hypothesis $H \models F$, we have $\mathcal{I} \models F$, so $\mathcal{I} \models H \rightarrow F$. In both cases, $\mathcal{I} \models H \rightarrow F$.

\textbf{($\Leftarrow$)} Conversely, suppose $H \rightarrow F$ is valid. Then for all interpretations $\mathcal{I}$, we have $\mathcal{I} \models H \rightarrow F$. By the semantics of implication, for all $\mathcal{I}$, either $\mathcal{I} \not\models H$ or $\mathcal{I} \models F$. Therefore, for all $\mathcal{I}$ such that $\mathcal{I} \models H$, we must have $\mathcal{I} \models F$, which is precisely the definition of $H \models F$.
\end{proof}

\subsection{Notational Ambiguity of $\models$}

The symbol $\models$ is used in three distinct but related contexts:

\begin{itemize}
    \item \textbf{Satisfaction}: $\mathcal{I} \models F$ means interpretation $\mathcal{I}$ satisfies formula $F$
    \item \textbf{Validity}: $\models F$ means formula $F$ is valid (satisfied by all interpretations)
    \item \textbf{Entailment}: $H \models F$ means formula $F$ is a logical consequence of the set of formulas $H$
\end{itemize}

While this overloading may initially seem confusing, these three uses are semantically coherent and represent different aspects of the same fundamental semantic relation.

\subsection{Reduction to Unsatisfiability}

A fundamental result connects logical consequence to unsatisfiability, providing another characterization of entailment.

\begin{theorem}[Logical Consequence via Unsatisfiability]
Let $H$ be a set of formulas and $F$ be a formula. Then:
\[
H \models F \quad \text{iff} \quad H \cup \{\neg F\} \text{ is unsatisfiable}
\]
\end{theorem}

\begin{proof}
We prove both directions:

\textbf{($\Rightarrow$)} Assume $H \models F$. We must show that $H \cup \{\neg F\}$ is unsatisfiable.

Suppose for contradiction that there exists an interpretation $\mathcal{I}$ such that $\mathcal{I} \models H \cup \{\neg F\}$. Then $\mathcal{I} \models H$ and $\mathcal{I} \models \neg F$, which implies $\mathcal{I} \not\models F$. However, since $H \models F$ and $\mathcal{I} \models H$, we must have $\mathcal{I} \models F$, which is a contradiction. Therefore, $H \cup \{\neg F\}$ is unsatisfiable.

\textbf{($\Leftarrow$)} Assume $H \cup \{\neg F\}$ is unsatisfiable. We must show that $H \models F$.

Let $\mathcal{I}$ be any interpretation such that $\mathcal{I} \models H$. Since $H \cup \{\neg F\}$ is unsatisfiable, there is no interpretation that satisfies both $H$ and $\neg F$. Therefore, $\mathcal{I} \not\models \neg F$, which implies $\mathcal{I} \models F$. Since this holds for all interpretations satisfying $H$, we conclude that $H \models F$.
\end{proof}

\section{Refutational Reasoning}

\subsection{Proof by Contradiction}

\textbf{Refutational reasoning} (or \textbf{proof by refutation}) is a fundamental technique in automated reasoning based on the principle of \textit{reductio ad absurdum}. To prove that a conclusion $F$ follows from hypotheses $H$, we assume the negation of the conclusion and derive a contradiction.

The method proceeds as follows:
\begin{enumerate}
    \item \textbf{Goal}: Prove $H \models F$ (where $H$ represents assumptions and $F$ represents the conjecture)
    \item \textbf{Method}: Show that $H \cup \{\neg F\}$ is unsatisfiable
    \item \textbf{Conclusion}: If $H \cup \{\neg F\}$ is unsatisfiable, then $H \models F$ by the previous theorem
\end{enumerate}

\subsection{The Refutational Framework}

The automated reasoning process can be formalized as follows:

\begin{center}
\begin{tikzpicture}[node distance=3cm, auto]
    \node (input) {$H \cup \{\neg F\}$};
    \node (procedure) [rectangle, draw, right of=input, node distance=4cm] {Decision Procedure};
    \node (unsat) [below right of=procedure, node distance=3cm] {\textbf{Unsatisfiable}};
    \node (sat) [above right of=procedure, node distance=3cm] {\textbf{Satisfiable}};
    
    \draw[->] (input) -- (procedure);
    \draw[->] (procedure) -- node[near start, above] {Yes} (unsat);
    \draw[->] (procedure) -- node[near start, above] {No} (sat);
    
    \node[right of=unsat, node distance=3.5cm, text width=3.5cm] {Proof found (refutation): $H \models F$};
    \node[right of=sat, node distance=3.5cm, text width=3.5cm] {Model found: counterexample to $H \models F$};
\end{tikzpicture}
\end{center}

\textbf{Two possible outcomes}:
\begin{itemize}
    \item \textbf{Unsatisfiable}: The procedure derives a contradiction, producing a \textbf{refutation} (proof) that $H \models F$
    \item \textbf{Satisfiable}: The procedure finds a model $\mathcal{I}$ of $H \cup \{\neg F\}$, which serves as a \textbf{counterexample} showing that $H \not\models F$
\end{itemize}

This refutational approach forms the foundation of modern automated theorem provers and SAT solvers, reducing the problem of proving logical consequence to the problem of detecting unsatisfiability.































\end{document}